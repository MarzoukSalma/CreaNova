# backend/services/rag_engine.py

from typing import Dict
from .retrieval import RetrievalService
from .llm.llmService import LLMService
from .llm.llmHelper import LLMHelper
from .memory import ConversationMemory


class RAGEngine:
    """
    Orchestrateur principal du systÃ¨me RAG.
    GÃ¨re :
    - conversation naturelle
    - gÃ©nÃ©ration crÃ©ative
    - questions basÃ©es sur la connaissance (RAG)
    """

    def __init__(self, db_connection_string: str, groq_api_key: str):
        print("ğŸ”§ Initialisation du moteur RAG...\n")

        self.retrieval = RetrievalService(db_connection_string)
        self.llm = LLMService(api_key=groq_api_key)
        self.llm_helper = LLMHelper()
        self.memory = ConversationMemory(max_history=5)

        print("\nâœ… Moteur RAG prÃªt !\n")

    # ============================================================
    # ğŸ§  INTENT DETECTION (LLM-BASED, SANS HARDCODE)
    # ============================================================
    def detect_intent(self, message: str) -> str:
        """
        DÃ©tecte l'intention utilisateur :
        - CONVERSATION
        - GENERATIVE
        - KNOWLEDGE
        """

        prompt = f"""
Tu es un routeur intelligent pour un assistant IA.

Classe le message utilisateur dans UNE SEULE catÃ©gorie :

1. CONVERSATION
   - salutations
   - remerciements
   - small talk
   - phrases sociales

2. GENERATIVE
   - demander des idÃ©es
   - demander des conseils
   - demander des suggestions
   - crÃ©ativitÃ© / brainstorming

3. KNOWLEDGE
   - question factuelle
   - question nÃ©cessitant des informations issues de documents

Message utilisateur :
"{message}"

RÃ©ponds STRICTEMENT par un seul mot :
CONVERSATION, GENERATIVE ou KNOWLEDGE
"""

        response = self.llm.ask(prompt).strip().upper()

        if response not in ["CONVERSATION", "GENERATIVE", "KNOWLEDGE"]:
            # fallback de sÃ©curitÃ©
            return "GENERATIVE"

        return response

    # ============================================================
    # ğŸ¯ MÃ‰THODE PRINCIPALE
    # ============================================================
    def ask(self, question: str, use_memory: bool = True, top_k: int = 3) -> Dict:
        """
        Traite une question avec :
        - dÃ©tection d'intention
        - rÃ©ponse directe OU gÃ©nÃ©ration crÃ©ative OU RAG
        """

        # ğŸŸ¢ Ã‰TAPE 0 : DÃ©tection d'intention
        intent = self.detect_intent(question)
        print(f"ğŸ§­ Intention dÃ©tectÃ©e : {intent}")

        # ========================================================
        # ğŸŸ£ CONVERSATION (salut, merci, small talk)
        # ========================================================
        if intent == "CONVERSATION":
            answer = self.llm.ask(
                f"RÃ©ponds de maniÃ¨re naturelle, amicale et concise Ã  ce message : {question}"
            )

            if use_memory:
                self.memory.add_exchange(question, answer)

            return {
                "answer": answer,
                "sources": [],
                "chunks_used": 0,
                "chunks": []
            }

        # ========================================================
        # ğŸŸ¡ GENERATIVE (idÃ©es, conseils, crÃ©ativitÃ©)
        # ========================================================
        if intent == "GENERATIVE":
            answer = self.llm.ask(
                f"RÃ©ponds de maniÃ¨re crÃ©ative, utile et structurÃ©e Ã  la demande suivante : {question}"
            )

            if use_memory:
                self.memory.add_exchange(question, answer)

            return {
                "answer": answer,
                "sources": [],
                "chunks_used": 0,
                "chunks": []
            }

        # ========================================================
        # ğŸ”µ KNOWLEDGE â†’ PIPELINE RAG
        # ========================================================
        print("ğŸ” Recherche de chunks pertinents (RAG)...")

        chunks = self.retrieval.search(query=question, top_k=top_k)

        if not chunks:
            return {
                "answer": (
                    "Je nâ€™ai pas trouvÃ© dâ€™information pertinente dans ma base de "
                    "connaissances pour rÃ©pondre Ã  cette question."
                ),
                "sources": [],
                "chunks_used": 0,
                "chunks": []
            }

        print(f"   âœ“ {len(chunks)} chunks trouvÃ©s")

        chunk_contents = [c["content"] for c in chunks]

        context_memory = None
        if use_memory and not self.memory.is_empty():
            context_memory = self.memory.get_context()

        prompt = self.llm_helper.build_prompt(
            question=question,
            chunks=chunk_contents,
            memory=context_memory
        )

        print("ğŸ¤– GÃ©nÃ©ration de la rÃ©ponse avec RAG...")
        answer = self.llm.ask(prompt)

        if use_memory:
            self.memory.add_exchange(question, answer)

        return {
            "answer": answer,
            "sources": [
                {
                    "file": c["file"],
                    "similarity": round(c["similarity"] * 100, 1)
                }
                for c in chunks
            ],
            "chunks_used": len(chunks),
            "chunks": [
                {
                    "preview": c["content"][:150] + "..."
                    if len(c["content"]) > 150
                    else c["content"],
                    "similarity": round(c["similarity"] * 100, 1)
                }
                for c in chunks
            ]
        }

    # ============================================================
    # ğŸ§¹ UTILITAIRES
    # ============================================================
    def clear_memory(self):
        self.memory.clear()
        print("ğŸ§¹ MÃ©moire conversationnelle effacÃ©e")

    def get_database_stats(self) -> Dict:
        return self.retrieval.get_statistics()

    def close(self):
        self.retrieval.close()
        print("ğŸ‘‹ Connexions fermÃ©es")
