# backend/services/rag_engine.py

from typing import Dict, Optional
from .retrieval import RetrievalService
from .llm.llmService import LLMService
from .llm.llmHelper import LLMHelper
from .memory import ConversationMemory

class RAGEngine:
    """
    Orchestrateur principal du systÃ¨me RAG.
    Coordonne la recherche, la mÃ©moire et la gÃ©nÃ©ration de rÃ©ponse.
    
    C'est le "cerveau" qui utilise tous les autres services.
    """
    
    def __init__(self, db_connection_string: str, groq_api_key: str):
        """
        Initialise le moteur RAG complet.
        
        Args:
            db_connection_string: Connexion PostgreSQL
            groq_api_key: ClÃ© API Groq pour le LLM
        """
        print("ğŸ”§ Initialisation du moteur RAG...\n")
        
        # 1. Service de recherche vectorielle
        self.retrieval = RetrievalService(db_connection_string)
        
        # 2. Service LLM (Groq)
        self.llm = LLMService(api_key=groq_api_key)
        
        # 3. Helper pour construire les prompts
        self.llm_helper = LLMHelper()
        
        # 4. MÃ©moire conversationnelle
        self.memory = ConversationMemory(max_history=5)
        
        print("\nâœ… Moteur RAG prÃªt !\n")

    
    
    def ask(self, question: str, use_memory: bool = True, top_k: int = 3) -> Dict:
        """
        Traite une question complÃ¨te avec le systÃ¨me RAG.
        
        FLUX:
        1. Recherche des chunks pertinents dans la base
        2. RÃ©cupÃ¨re l'historique de conversation (si activÃ©)
        3. Construit un prompt optimisÃ©
        4. Envoie au LLM pour gÃ©nÃ©ration
        5. Sauvegarde l'Ã©change dans la mÃ©moire
        
        Args:
            question: Question de l'utilisateur
            use_memory: Utiliser l'historique de conversation (par dÃ©faut True)
            top_k: Nombre de chunks Ã  rÃ©cupÃ©rer (par dÃ©faut 3)
            
        Returns:
            Dictionnaire contenant:
                - answer: La rÃ©ponse gÃ©nÃ©rÃ©e
                - sources: Liste des sources utilisÃ©es avec similaritÃ©
                - chunks_used: Nombre de chunks utilisÃ©s
                - chunks: AperÃ§u des chunks (pour debug)
        """
        print(f"ğŸ” Recherche de chunks pertinents...")
        
        # Ã‰TAPE 1 : Recherche vectorielle
        chunks = self.retrieval.search(query=question, top_k=top_k)
        
        # VÃ©rifier si des rÃ©sultats ont Ã©tÃ© trouvÃ©s
        if not chunks:
            return {
                'answer': "DÃ©solÃ©, je n'ai pas trouvÃ© d'information pertinente dans ma base de connaissances pour rÃ©pondre Ã  cette question. Pouvez-vous reformuler ou poser une autre question ?",
                'sources': [],
                'chunks_used': 0,
                'chunks': []
            }
        
        print(f"   âœ“ {len(chunks)} chunks trouvÃ©s")
        
        # Ã‰TAPE 2 : Extraire le contenu des chunks
        chunk_contents = [c['content'] for c in chunks]
        
        # Ã‰TAPE 3 : RÃ©cupÃ©rer la mÃ©moire (optionnel)
        context_memory = None
        if use_memory and not self.memory.is_empty():
            context_memory = self.memory.get_context()
            print(f"   âœ“ Contexte de {self.memory.size()} Ã©change(s) prÃ©cÃ©dent(s) ajoutÃ©")
        
        # Ã‰TAPE 4 : Construire le prompt
        print(f"ğŸ“ Construction du prompt...")
        prompt = self.llm_helper.build_prompt(
            question=question,
            chunks=chunk_contents,
            memory=context_memory
        )
        
        # Ã‰TAPE 5 : GÃ©nÃ©rer la rÃ©ponse avec le LLM
        print(f"ğŸ¤– GÃ©nÃ©ration de la rÃ©ponse...")
        answer = self.llm.ask(prompt)
        
        # Ã‰TAPE 6 : Sauvegarder dans la mÃ©moire
        if use_memory:
            self.memory.add_exchange(question, answer)
        
        # Ã‰TAPE 7 : Retourner les rÃ©sultats complets
        return {
            'answer': answer,
            'sources': [
                {
                    'file': c['file'], 
                    'similarity': round(c['similarity'] * 100, 1)  # En pourcentage
                } 
                for c in chunks
            ],
            'chunks_used': len(chunks),
            'chunks': [
                {
                    'preview': c['content'][:150] + '...' if len(c['content']) > 150 else c['content'],
                    'similarity': round(c['similarity'] * 100, 1)
                }
                for c in chunks
            ]
        }
    
    def ask_without_rag(self, question: str) -> str:
        """
        Pose une question directement au LLM SANS utiliser la base RAG.
        Utile pour des questions gÃ©nÃ©rales.
        
        Args:
            question: Question de l'utilisateur
            
        Returns:
            RÃ©ponse du LLM
        """
        return self.llm.ask(question)
    
    def get_memory_summary(self) -> Dict:
        """
        Retourne un rÃ©sumÃ© de la mÃ©moire conversationnelle.
        
        Returns:
            Dictionnaire avec les statistiques de la mÃ©moire
        """
        return {
            'size': self.memory.size(),
            'max_history': self.memory.max_history,
            'is_empty': self.memory.is_empty(),
            'last_exchanges': self.memory.get_last_n(3)
        }
    
    def clear_memory(self):
        """Efface l'historique de conversation."""
        self.memory.clear()
        print("ğŸ§¹ MÃ©moire conversationnelle effacÃ©e")
    
    def get_database_stats(self) -> Dict:
        """
        Retourne les statistiques de la base de donnÃ©es.
        
        Returns:
            Dictionnaire avec le nombre de documents et chunks
        """
        return self.retrieval.get_statistics()
    
    def close(self):
        """Ferme toutes les connexions."""
        self.retrieval.close()
        print("ğŸ‘‹ Connexions fermÃ©es")


# Test du module
if __name__ == "__main__":
    print("=== TEST DU MOTEUR RAG COMPLET ===\n")
    
    # Initialisation
    engine = RAGEngine(
        db_connection_string="dbname=rag_chatbot_db user=postgres password=RAG_DB_PASSWORD",
        groq_api_key="votre_cle_api"  # Remplacez par votre vraie clÃ©
    )
    
    # Stats
    stats = engine.get_database_stats()
    print(f"ğŸ“Š Base: {stats['num_documents']} docs, {stats['num_chunks']} chunks\n")
    
    # Test 1
    print("=" * 60)
    result = engine.ask("Comment rester motivÃ© ?")
    print(f"\nğŸ¤– RÃ©ponse: {result['answer'][:200]}...")
    print(f"\nğŸ“š Sources: {result['sources']}")
    
    # Test 2 (avec mÃ©moire)
    print("\n" + "=" * 60)
    result = engine.ask("Donne-moi plus de dÃ©tails")
    print(f"\nğŸ¤– RÃ©ponse: {result['answer'][:200]}...")
    
    # Nettoyer
    engine.close()